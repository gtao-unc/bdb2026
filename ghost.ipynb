{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2088910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "# Data dimensions from Big Data Bowl (from dataclean.ipynb processing)\n",
    "T_HIST = 25         # Number of historical frames (max in dataset)\n",
    "T_PRED = 25         # Number of frames to predict (max in dataset)\n",
    "N_AGENTS = 9        # Actual number of agents per frame in data\n",
    "D_AGENT = 33        # Agent features: player_height, player_weight, s, a, dir, o, x_rel, y_rel + one-hot encoded position/side/role\n",
    "D_GLOBAL = 18       # Global features: down, yards_to_go + one-hot encoded dropback_type, team_coverage_type\n",
    "\n",
    "# Model architecture hyperparameters\n",
    "D_MODEL = 128       # Transformer Embedding Dimension\n",
    "D_LATENT = 32       # Latent variable Z dimension\n",
    "N_HEADS = 8         # Transformer Heads\n",
    "N_LAYERS = 3        # Transformer Encoder Layers\n",
    "KL_BETA = 0.01      # KL Loss Weight (needs tuning/annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589959a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeliocentricityTransformer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Unpack kwargs for clarity\n",
    "        self.T_HIST, self.T_PRED, self.N_AGENTS = kwargs['T_HIST'], kwargs['T_PRED'], kwargs['N_AGENTS']\n",
    "        self.D_AGENT, self.D_GLOBAL, self.D_MODEL = kwargs['D_AGENT'], kwargs['D_GLOBAL'], kwargs['D_MODEL']\n",
    "        self.D_LATENT, self.N_HEADS, self.N_LAYERS = kwargs['D_LATENT'], kwargs['N_HEADS'], kwargs['N_LAYERS']\n",
    "        self.KL_BETA = kwargs['KL_BETA']\n",
    "        \n",
    "        # --- 1. Initial Embedding Layers ---\n",
    "        self.agent_embed = nn.Linear(self.D_AGENT, self.D_MODEL)\n",
    "        self.global_embed = nn.Linear(self.D_GLOBAL, self.D_MODEL)\n",
    "        \n",
    "        # --- 2. Transformer Encoder (Core STT) ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.D_MODEL, \n",
    "            nhead=self.N_HEADS, \n",
    "            dim_feedforward=self.D_MODEL * 4, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.N_LAYERS)\n",
    "        \n",
    "        # --- 3. CVAE Heads (Prediction Heads from Context C) ---\n",
    "        # CAVE requires a context vector (C) for prior/recognition networks\n",
    "        \n",
    "        # CVAE: Prior Network (p(z|C)) -> outputs mu_prior, log_var_prior\n",
    "        self.mlp_prior = nn.Sequential(\n",
    "            nn.Linear(self.D_MODEL, self.D_MODEL),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.D_MODEL, 2 * self.D_LATENT)\n",
    "        )\n",
    "\n",
    "        # CVAE: Recognition Network (q(z|C, Y_truth)) -> outputs mu_rec, log_var_rec\n",
    "        # Input is C + flattened Y_truth (context + ground truth trajectory)\n",
    "        self.mlp_recognition = nn.Sequential(\n",
    "            nn.Linear(self.D_MODEL + self.T_PRED * self.N_AGENTS * 2, self.D_MODEL),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.D_MODEL, 2 * self.D_LATENT)\n",
    "        )\n",
    "\n",
    "        # --- 4. Decoder Head (Trajectory Generator) ---\n",
    "        # Input is C + Z. Output is the flattened trajectory (x, y coordinates)\n",
    "        self.mlp_decoder = nn.Sequential(\n",
    "            nn.Linear(self.D_MODEL + self.D_LATENT, self.D_MODEL * 2),\n",
    "            nn.ReLU(),\n",
    "            # Output shape: (Batch, T_PRED * N_AGENTS * 2)\n",
    "            nn.Linear(self.D_MODEL * 2, self.T_PRED * self.N_AGENTS * 2)\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        # Sampling Z = mu + sigma * epsilon\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, X_hist_agents, X_global, Y_truth=None):\n",
    "        B = X_hist_agents.size(0)\n",
    "        \n",
    "        # 1. Agent Embedding (Per-Frame)\n",
    "        # (B, T_hist, N_agents, D_agent) -> (B, T_hist, N_agents, D_MODEL)\n",
    "        agent_emb = self.agent_embed(X_hist_agents)\n",
    "        \n",
    "        # 2. Global CLS Token Embedding\n",
    "        # (B, D_global) -> (B, D_MODEL)\n",
    "        global_emb = self.global_embed(X_global)\n",
    "        # Expand for T_hist: (B, 1, 1, D_MODEL). Expand(T_hist) not needed as we use flatten below\n",
    "        \n",
    "        # 3. Prepare Sequence for Transformer\n",
    "        \n",
    "        # Create CLS Token for each time step in the historical sequence\n",
    "        # Shape: (B, T_hist, 1, D_MODEL)\n",
    "        cls_tokens = global_emb.unsqueeze(1).unsqueeze(1).expand(-1, self.T_HIST, -1, -1)\n",
    "        \n",
    "        # Concatenate CLS token to the front of each frame's set of agents\n",
    "        # Shape: (B, T_hist, N_agents + 1, D_MODEL)\n",
    "        input_sequence = torch.cat([cls_tokens, agent_emb], dim=2)\n",
    "        \n",
    "        # Flatten time and agent dimensions for Transformer input\n",
    "        # Shape: (B, T_hist * (N_agents + 1), D_MODEL)\n",
    "        flat_input = input_sequence.view(B, -1, self.D_MODEL)\n",
    "        \n",
    "        # Add Positional/Temporal Encoding here (Omitted)\n",
    "        \n",
    "        # 4. Transformer Encoding\n",
    "        # Encoded_Output: (B, T_hist * (N_agents + 1), D_MODEL)\n",
    "        encoded_output = self.transformer_encoder(flat_input)\n",
    "        \n",
    "        # 5. Extract Context Vector C from the first CLS token\n",
    "        # The first token is CLS at t=0. C should capture the full context.\n",
    "        # Context C: (B, D_MODEL)\n",
    "        C = encoded_output[:, 0, :]\n",
    "        \n",
    "        # --- CVAE Latent Space ---\n",
    "        # Prior Network: p(z|C)\n",
    "        mu_prior, log_var_prior = self.mlp_prior(C).chunk(2, dim=-1)\n",
    "\n",
    "        # Recognition Network: q(z|C, Y_truth) is only used during training\n",
    "        if Y_truth is not None:\n",
    "            # Flatten Y_truth: (B, T_pred * N_agents * 2)\n",
    "            Y_flat = Y_truth.view(B, -1)\n",
    "            rec_input = torch.cat([C, Y_flat], dim=-1)\n",
    "            mu_rec, log_var_rec = self.mlp_recognition(rec_input).chunk(2, dim=-1)\n",
    "            Z = self.reparameterize(mu_rec, log_var_rec)\n",
    "        else:\n",
    "            # Inference: Sample Z from the Prior distribution p(z|C)\n",
    "            # This is key for generating diverse, expected trajectories (E)\n",
    "            Z = self.reparameterize(mu_prior, log_var_prior)\n",
    "            mu_rec, log_var_rec = mu_prior, log_var_prior # Use prior stats for loss calc placeholder\n",
    "\n",
    "        # --- Decoder ---\n",
    "        # Input: [C, Z]\n",
    "        decoder_input = torch.cat([C, Z], dim=-1)\n",
    "        \n",
    "        # Output: (B, T_pred * N_agents * 2)\n",
    "        Y_pred_flat = self.mlp_decoder(decoder_input)\n",
    "        \n",
    "        # Reshape to (B, T_pred, N_agents, 2)\n",
    "        Y_pred = Y_pred_flat.view(B, self.T_PRED, self.N_AGENTS, 2)\n",
    "        \n",
    "        return Y_pred, mu_rec, log_var_rec, mu_prior, log_var_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7b44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(Y_pred, Y_truth, mu_rec, log_var_rec, mu_prior, log_var_prior, KL_BETA):\n",
    "    # 1. Reconstruction Loss (L_recon): RMSE on the predicted x, y coordinates\n",
    "    # We use MSE here for simplicity in PyTorch, but RMSE is the metric.\n",
    "    L_recon = F.mse_loss(Y_pred, Y_truth, reduction='sum') / Y_pred.size(0) # Mean over batch\n",
    "\n",
    "    # 2. KL Divergence Loss (L_KL): KL(q(z|C, Y) || p(z|C))\n",
    "    # Closed-form KL for Gaussian: 0.5 * sum(1 + log(sigma_prior^2) - log(sigma_rec^2) - (mu_rec - mu_prior)^2 / sigma_prior^2 - exp(log(sigma_rec^2)) / sigma_prior^2)\n",
    "    # Using torch.exp(log_var) = sigma^2\n",
    "    kl_loss = 0.5 * torch.sum(\n",
    "        log_var_prior - log_var_rec - 1 \n",
    "        + (torch.exp(log_var_rec) + (mu_rec - mu_prior).pow(2)) / torch.exp(log_var_prior)\n",
    "    ) / Y_pred.size(0)\n",
    "\n",
    "    # Total Loss (Weighted sum)\n",
    "    total_loss = L_recon + KL_BETA * kl_loss\n",
    "    return total_loss, L_recon.item(), kl_loss.item()\n",
    "\n",
    "# --- Heliocentricity Inference Function (E Generator) ---\n",
    "@torch.no_grad()\n",
    "def generate_expected_trajectories(model, X_hist_agents, X_global, K=10):\n",
    "    \"\"\"\n",
    "    Generates K diverse, plausible trajectories for the defense (E) \n",
    "    by sampling the latent space Z from the prior distribution.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    B = X_hist_agents.size(0)\n",
    "    \n",
    "    # Repeat inputs K times to batch the K samples\n",
    "    X_hist_agents_K = X_hist_agents.repeat_interleave(K, dim=0)\n",
    "    X_global_K = X_global.repeat_interleave(K, dim=0)\n",
    "\n",
    "    # Since Y_truth=None, Z is sampled from the prior p(z|C)\n",
    "    Y_pred_K, _, _, _, _ = model(X_hist_agents_K, X_global_K, Y_truth=None)\n",
    "    \n",
    "    # Reshape: (B * K, T_pred, N_agents, 2) -> (B, K, T_pred, N_agents, 2)\n",
    "    return Y_pred_K.view(B, K, model.T_PRED, model.N_AGENTS, 2)\n",
    "\n",
    "# Note: The final step of calculating Heliocentricity (H) based on \n",
    "# min separation distance (A vs E) is a NumPy/Pandas operation after this PyTorch step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7d27a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/processed/processed_data.pt...\n",
      "Loaded 14108 plays\n",
      "Global context shape: torch.Size([14108, 18])\n",
      "Created DataLoader with 14108 samples, batch size 32\n",
      "Max hist length: 25, Max pred length: 25, Max agents: 9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Setup: Instantiate Model and Optimizer ---\n",
    "\n",
    "# Define a dictionary for easy configuration\n",
    "model_config = {\n",
    "    'T_HIST': 25, 'T_PRED': 25, 'N_AGENTS': 9, 'D_AGENT': 33, \n",
    "    'D_GLOBAL': 18, 'D_MODEL': 128, 'D_LATENT': 32, 'N_HEADS': 8, \n",
    "    'N_LAYERS': 3, 'KL_BETA': 0.01 \n",
    "}\n",
    "\n",
    "# Instantiate the model and move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HeliocentricityTransformer(**model_config).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- 2. Load Big Data Bowl Data from Disk ---\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load processed data\n",
    "data_path = Path('dataset/processed/processed_data.pt')\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "loaded_data = torch.load(data_path)\n",
    "\n",
    "# Extract data\n",
    "historical_agent_features = loaded_data['historical_agent_features']\n",
    "ground_truth_trajectories = loaded_data['ground_truth_trajectories']\n",
    "global_context_features = loaded_data['global_context_features']\n",
    "\n",
    "print(f\"Loaded {len(historical_agent_features)} plays\")\n",
    "print(f\"Global context shape: {global_context_features.shape}\")\n",
    "\n",
    "# Custom Dataset with Padding (both time and agent dimensions)\n",
    "class FootballDataset(Dataset):\n",
    "    def __init__(self, hist_features, gt_trajectories, global_features, max_hist_len=None, max_pred_len=None, max_n_agents=None):\n",
    "        self.hist_features = hist_features\n",
    "        self.gt_trajectories = gt_trajectories\n",
    "        self.global_features = global_features\n",
    "        \n",
    "        # Determine max lengths if not provided\n",
    "        self.max_hist_len = max_hist_len or max(x.shape[0] for x in hist_features)\n",
    "        self.max_pred_len = max_pred_len or max(y.shape[0] for y in gt_trajectories)\n",
    "        self.max_n_agents = max_n_agents or max(x.shape[1] for x in hist_features)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hist_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hist = self.hist_features[idx]  # (T_hist_actual, N_agents_actual, D_agent)\n",
    "        gt = self.gt_trajectories[idx]  # (T_pred_actual, N_agents_actual, 2)\n",
    "        global_feat = self.global_features[idx]  # (D_global,)\n",
    "        \n",
    "        # Get actual lengths\n",
    "        hist_len = hist.shape[0]\n",
    "        pred_len = gt.shape[0]\n",
    "        n_agents = hist.shape[1]\n",
    "        \n",
    "        # Pad historical features to max_hist_len and max_n_agents\n",
    "        # First pad time dimension\n",
    "        if hist_len < self.max_hist_len:\n",
    "            pad_hist_time = torch.zeros(self.max_hist_len - hist_len, hist.shape[1], hist.shape[2], dtype=hist.dtype)\n",
    "            hist = torch.cat([hist, pad_hist_time], dim=0)\n",
    "        else:\n",
    "            hist = hist[:self.max_hist_len]\n",
    "            hist_len = self.max_hist_len\n",
    "        \n",
    "        # Then pad agent dimension\n",
    "        if n_agents < self.max_n_agents:\n",
    "            pad_hist_agents = torch.zeros(hist.shape[0], self.max_n_agents - n_agents, hist.shape[2], dtype=hist.dtype)\n",
    "            hist_padded = torch.cat([hist, pad_hist_agents], dim=1)\n",
    "        else:\n",
    "            hist_padded = hist[:, :self.max_n_agents, :]\n",
    "            n_agents = self.max_n_agents\n",
    "        \n",
    "        # Pad ground truth to max_pred_len and max_n_agents\n",
    "        # First pad time dimension\n",
    "        if pred_len < self.max_pred_len:\n",
    "            pad_gt_time = torch.zeros(self.max_pred_len - pred_len, gt.shape[1], 2, dtype=gt.dtype)\n",
    "            gt = torch.cat([gt, pad_gt_time], dim=0)\n",
    "        else:\n",
    "            gt = gt[:self.max_pred_len]\n",
    "            pred_len = self.max_pred_len\n",
    "        \n",
    "        # Then pad agent dimension\n",
    "        if gt.shape[1] < self.max_n_agents:\n",
    "            pad_gt_agents = torch.zeros(gt.shape[0], self.max_n_agents - gt.shape[1], 2, dtype=gt.dtype)\n",
    "            gt_padded = torch.cat([gt, pad_gt_agents], dim=1)\n",
    "        else:\n",
    "            gt_padded = gt[:, :self.max_n_agents, :]\n",
    "        \n",
    "        return hist_padded, global_feat, gt_padded, hist_len, pred_len\n",
    "\n",
    "# Create dataset with padding\n",
    "# Use model config values as max lengths\n",
    "dataset = FootballDataset(\n",
    "    historical_agent_features, \n",
    "    ground_truth_trajectories, \n",
    "    global_context_features,\n",
    "    max_hist_len=model_config['T_HIST'],\n",
    "    max_pred_len=model_config['T_PRED'],\n",
    "    max_n_agents=model_config['N_AGENTS']\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Created DataLoader with {len(dataset)} samples, batch size {BATCH_SIZE}\")\n",
    "print(f\"Max hist length: {dataset.max_hist_len}, Max pred length: {dataset.max_pred_len}, Max agents: {dataset.max_n_agents}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37835750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. The Training Loop with Masking ---\n",
    "\n",
    "def create_mask(lengths, max_len, device):\n",
    "    \"\"\"Create attention mask: True for valid positions, False for padding\"\"\"\n",
    "    batch_size = len(lengths)\n",
    "    mask = torch.arange(max_len, device=device).expand(batch_size, max_len) < lengths.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "def train_model(model, train_loader, optimizer, model_config, device, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Train the Heliocentricity Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        model: The HeliocentricityTransformer model\n",
    "        train_loader: DataLoader for training data\n",
    "        optimizer: Optimizer for training\n",
    "        model_config: Dictionary with model configuration\n",
    "        device: Device to train on (cuda/cpu)\n",
    "        num_epochs: Number of epochs to train\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    print(f\"Starting training on {device}...\")\n",
    "    \n",
    "    history = {\n",
    "        'total_loss': [],\n",
    "        'recon_loss': [],\n",
    "        'kl_loss': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_kl_loss = 0\n",
    "        \n",
    "        for batch_idx, (X_agents, X_global, Y_truth, hist_lens, pred_lens) in enumerate(train_loader):\n",
    "            \n",
    "            # Move to device\n",
    "            X_agents = X_agents.to(device)\n",
    "            X_global = X_global.to(device)\n",
    "            Y_truth = Y_truth.to(device)\n",
    "            hist_lens = hist_lens.to(device)\n",
    "            pred_lens = pred_lens.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 1. Forward Pass\n",
    "            # The forward pass uses the recognition network q(z|C, Y_truth) since Y_truth is provided.\n",
    "            Y_pred, mu_rec, log_var_rec, mu_prior, log_var_prior = model(X_agents, X_global, Y_truth=Y_truth)\n",
    "            \n",
    "            # 2. Create mask for ground truth prediction loss\n",
    "            # Only compute loss on valid (non-padded) timesteps\n",
    "            pred_mask = create_mask(pred_lens, model_config['T_PRED'], device)\n",
    "            # Expand mask to match Y_pred shape: (B, T_pred, N_agents, 2)\n",
    "            pred_mask_expanded = pred_mask.unsqueeze(-1).unsqueeze(-1).expand_as(Y_pred)\n",
    "            \n",
    "            # Apply mask to predictions and ground truth\n",
    "            Y_pred_masked = Y_pred * pred_mask_expanded\n",
    "            Y_truth_masked = Y_truth * pred_mask_expanded\n",
    "            \n",
    "            # 3. Compute VAE Loss with masked outputs\n",
    "            # Reconstruction loss only on valid timesteps\n",
    "            L_recon = F.mse_loss(Y_pred_masked, Y_truth_masked, reduction='sum') / pred_lens.sum()\n",
    "            \n",
    "            # KL loss (not masked, as it's based on latent distribution)\n",
    "            kl_loss = 0.5 * torch.sum(\n",
    "                log_var_prior - log_var_rec - 1 \n",
    "                + (torch.exp(log_var_rec) + (mu_rec - mu_prior).pow(2)) / torch.exp(log_var_prior)\n",
    "            ) / X_agents.size(0)\n",
    "            \n",
    "            loss = L_recon + model_config['KL_BETA'] * kl_loss\n",
    "            \n",
    "            # 4. Backward Pass and Optimization\n",
    "            loss.backward()\n",
    "            # Optional: Gradient clipping to stabilize Transformer training\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0) \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += L_recon.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            \n",
    "            # Print update every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"  Batch {batch_idx+1}/{len(train_loader)} | Total Loss: {total_loss / (batch_idx+1):.4f} | Recon: {total_recon_loss / (batch_idx+1):.4f} | KL: {total_kl_loss / (batch_idx+1):.4f}\")\n",
    "    \n",
    "        # --- End of Epoch ---\n",
    "        avg_epoch_loss = total_loss / len(train_loader)\n",
    "        avg_recon = total_recon_loss / len(train_loader)\n",
    "        avg_kl = total_kl_loss / len(train_loader)\n",
    "        \n",
    "        history['total_loss'].append(avg_epoch_loss)\n",
    "        history['recon_loss'].append(avg_recon)\n",
    "        history['kl_loss'].append(avg_kl)\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} Complete ---\")\n",
    "        print(f\"Average Total Loss: {avg_epoch_loss:.4f} | Recon: {avg_recon:.4f} | KL: {avg_kl:.4f}\")\n",
    "        \n",
    "        # Optional: Implement a validation loop here and save the best model weights\n",
    "        # torch.save(model.state_dict(), f\"best_heliocentricity_model.pt\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run training\n",
    "# training_history = train_model(model, train_loader, optimizer, model_config, device, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5cfcb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the saved model weights\n",
    "# model.load_state_dict(torch.load(\"best_heliocentricity_model.pt\"))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_rmse = []\n",
    "    total_recon_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    \n",
    "    # Store data for final Heliocentricity calculation outside the loop\n",
    "    results_for_H_calc = []\n",
    "\n",
    "    for batch_idx, (X_agents, X_global, Y_truth, hist_lens, pred_lens) in enumerate(train_loader):\n",
    "            \n",
    "        # Move to device\n",
    "        X_agents = X_agents.to(device)\n",
    "        X_global = X_global.to(device)\n",
    "        Y_truth = Y_truth.to(device)\n",
    "        hist_lens = hist_lens.to(device)\n",
    "        pred_lens = pred_lens.to(device)\n",
    "\n",
    "        # 1. Deterministic Prediction (for Reconstruction Loss)\n",
    "        # Uses the recognition network q(z|C, Y_truth) which yields the best reconstruction\n",
    "        Y_pred, mu_rec, log_var_rec, mu_prior, log_var_prior = model(X_agents, X_global, Y_truth=Y_truth)\n",
    "        \n",
    "        # Calculate Loss components\n",
    "        loss, L_recon, L_KL = vae_loss(\n",
    "            Y_pred, Y_truth, \n",
    "            mu_rec, log_var_rec, mu_prior, log_var_prior, \n",
    "            model.KL_BETA\n",
    "        )\n",
    "        total_recon_loss += L_recon\n",
    "        total_kl_loss += L_KL\n",
    "\n",
    "        # 2. Calculate Root Mean Squared Error (RMSE) on the deterministic prediction\n",
    "        # Detach and convert to numpy for standard metric calculation\n",
    "        Y_pred_np = Y_pred.cpu().numpy()\n",
    "        Y_truth_np = Y_truth.cpu().numpy()\n",
    "        \n",
    "        # Calculate RMSE for each sample and average (Flattening all T_pred * N_agents * 2 dimensions)\n",
    "        sample_rmse = np.sqrt(mean_squared_error(Y_truth_np.reshape(-1, 1), Y_pred_np.reshape(-1, 1)))\n",
    "        total_rmse.append(sample_rmse)\n",
    "\n",
    "        # 3. Generate K Stochastic Predictions for Heliocentricity (E)\n",
    "        # This uses the prior network p(z|C) for diverse sampling\n",
    "        K = 10 # Number of samples per play\n",
    "        Y_pred_K = generate_expected_trajectories(model, X_agents, X_global, K=K).cpu().numpy()\n",
    "        \n",
    "        # Store results (You would need to include the actual Star Receiver ID/Index here)\n",
    "        for i in range(Y_truth_np.shape[0]):\n",
    "            results_for_H_calc.append({\n",
    "                'Y_truth': Y_truth_np[i],\n",
    "                'Y_pred_K': Y_pred_K[i],\n",
    "                # Assume you have a way to link back to the play ID and the Star Receiver Index\n",
    "                'star_idx': 4 # Pseudo-Index for the star receiver\n",
    "            })\n",
    "\n",
    "        # Print update every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Batch {batch_idx+1}/{len(train_loader)}\")\n",
    "\n",
    "    avg_rmse = np.mean(total_rmse)\n",
    "    avg_recon = total_recon_loss / len(data_loader)\n",
    "    avg_kl = total_kl_loss / len(data_loader)\n",
    "    \n",
    "    print(f\"\\n--- Validation Results ---\")\n",
    "    print(f\"Avg Trajectory RMSE: {avg_rmse:.4f} meters\")\n",
    "    print(f\"Avg Reconstruction Loss: {avg_recon:.4f}\")\n",
    "    print(f\"Avg KL Divergence: {avg_kl:.4f}\")\n",
    "    \n",
    "    return results_for_H_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba5a80a-a9f6-4e40-ab56-aa900bcc8112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/processed/processed_data.pt...\n",
      "Loaded 14108 plays\n",
      "Global context shape: torch.Size([14108, 18])\n",
      "train_size: 11286\n",
      "test_size: 2822\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Setup: Instantiate Model and Optimizer ---\n",
    "\n",
    "# Define a dictionary for easy configuration\n",
    "model_config = {\n",
    "    'T_HIST': 25, 'T_PRED': 25, 'N_AGENTS': 9, 'D_AGENT': 33, \n",
    "    'D_GLOBAL': 18, 'D_MODEL': 128, 'D_LATENT': 32, 'N_HEADS': 8, \n",
    "    'N_LAYERS': 3, 'KL_BETA': 0.01 \n",
    "}\n",
    "\n",
    "# Instantiate the model and move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HeliocentricityTransformer(**model_config).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# --- 2. Load Big Data Bowl Data from Disk ---\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load processed data\n",
    "data_path = Path('dataset/processed/processed_data.pt')\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "loaded_data = torch.load(data_path)\n",
    "\n",
    "# Extract data\n",
    "historical_agent_features = loaded_data['historical_agent_features']\n",
    "ground_truth_trajectories = loaded_data['ground_truth_trajectories']\n",
    "global_context_features = loaded_data['global_context_features']\n",
    "\n",
    "print(f\"Loaded {len(historical_agent_features)} plays\")\n",
    "print(f\"Global context shape: {global_context_features.shape}\")\n",
    "\n",
    "# Custom Dataset with Padding (both time and agent dimensions)\n",
    "class FootballDataset(Dataset):\n",
    "    def __init__(self, hist_features, gt_trajectories, global_features, max_hist_len=None, max_pred_len=None, max_n_agents=None):\n",
    "        self.hist_features = hist_features\n",
    "        self.gt_trajectories = gt_trajectories\n",
    "        self.global_features = global_features\n",
    "        \n",
    "        # Determine max lengths if not provided\n",
    "        self.max_hist_len = max_hist_len or max(x.shape[0] for x in hist_features)\n",
    "        self.max_pred_len = max_pred_len or max(y.shape[0] for y in gt_trajectories)\n",
    "        self.max_n_agents = max_n_agents or max(x.shape[1] for x in hist_features)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hist_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hist = self.hist_features[idx]  # (T_hist_actual, N_agents_actual, D_agent)\n",
    "        gt = self.gt_trajectories[idx]  # (T_pred_actual, N_agents_actual, 2)\n",
    "        global_feat = self.global_features[idx]  # (D_global,)\n",
    "        \n",
    "        # Get actual lengths\n",
    "        hist_len = hist.shape[0]\n",
    "        pred_len = gt.shape[0]\n",
    "        n_agents = hist.shape[1]\n",
    "        \n",
    "        # Pad historical features to max_hist_len and max_n_agents\n",
    "        # First pad time dimension\n",
    "        if hist_len < self.max_hist_len:\n",
    "            pad_hist_time = torch.zeros(self.max_hist_len - hist_len, hist.shape[1], hist.shape[2], dtype=hist.dtype)\n",
    "            hist = torch.cat([hist, pad_hist_time], dim=0)\n",
    "        else:\n",
    "            hist = hist[:self.max_hist_len]\n",
    "            hist_len = self.max_hist_len\n",
    "        \n",
    "        # Then pad agent dimension\n",
    "        if n_agents < self.max_n_agents:\n",
    "            pad_hist_agents = torch.zeros(hist.shape[0], self.max_n_agents - n_agents, hist.shape[2], dtype=hist.dtype)\n",
    "            hist_padded = torch.cat([hist, pad_hist_agents], dim=1)\n",
    "        else:\n",
    "            hist_padded = hist[:, :self.max_n_agents, :]\n",
    "            n_agents = self.max_n_agents\n",
    "        \n",
    "        # Pad ground truth to max_pred_len and max_n_agents\n",
    "        # First pad time dimension\n",
    "        if pred_len < self.max_pred_len:\n",
    "            pad_gt_time = torch.zeros(self.max_pred_len - pred_len, gt.shape[1], 2, dtype=gt.dtype)\n",
    "            gt = torch.cat([gt, pad_gt_time], dim=0)\n",
    "        else:\n",
    "            gt = gt[:self.max_pred_len]\n",
    "            pred_len = self.max_pred_len\n",
    "        \n",
    "        # Then pad agent dimension\n",
    "        if gt.shape[1] < self.max_n_agents:\n",
    "            pad_gt_agents = torch.zeros(gt.shape[0], self.max_n_agents - gt.shape[1], 2, dtype=gt.dtype)\n",
    "            gt_padded = torch.cat([gt, pad_gt_agents], dim=1)\n",
    "        else:\n",
    "            gt_padded = gt[:, :self.max_n_agents, :]\n",
    "        \n",
    "        return hist_padded, global_feat, gt_padded, hist_len, pred_len\n",
    "\n",
    "# Create dataset with padding\n",
    "# Use model config values as max lengths\n",
    "dataset = FootballDataset(\n",
    "    historical_agent_features, \n",
    "    ground_truth_trajectories, \n",
    "    global_context_features,\n",
    "    max_hist_len=model_config['T_HIST'],\n",
    "    max_pred_len=model_config['T_PRED'],\n",
    "    max_n_agents=model_config['N_AGENTS']\n",
    ")\n",
    "\n",
    "# Define the desired lengths for the train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Perform the split\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f'train_size: {train_size}\\ntest_size: {test_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26748931-5208-439f-abbc-e3b20738884b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from pretrained weights:\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_WGTS = Path('dataset/pretrained/best_heliocentricity_model.pt')\n",
    "\n",
    "if PRETRAINED_WGTS is None:\n",
    "    print('Training model from scratch:')\n",
    "    train_model(model,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                model_config,\n",
    "                device,\n",
    "                num_epochs=NUM_EPOCHS)\n",
    "    torch.save(model.state_dict(), 'dataset/pretrained/best_heliocentricity_model.pt')\n",
    "else:\n",
    "    print('Loading model from pretrained weights:')\n",
    "    state_dict = torch.load(PRETRAINED_WGTS, map_location=device)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3decea14-883c-42c4-8015-1b4f62bd2deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/177\n",
      "\n",
      "--- Validation Results ---\n",
      "Avg Trajectory RMSE: 2.8821 meters\n",
      "Avg Reconstruction Loss: 14763.0471\n",
      "Avg KL Divergence: 608.6204\n"
     ]
    }
   ],
   "source": [
    "pred = evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a935014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_separation_distance(receiver_coords, defense_coords):\n",
    "    # receiver_coords: (T_pred, 2)\n",
    "    # defense_coords: (T_pred, N_defenders, 2)\n",
    "    \n",
    "    # Calculate distance from receiver to every defender at every frame\n",
    "    # (T_pred, N_defenders)\n",
    "    dist_to_defenders = np.linalg.norm(\n",
    "        receiver_coords[:, np.newaxis, :] - defense_coords, axis=2\n",
    "    )\n",
    "    \n",
    "    # Find the minimum separation at each frame (T_pred,)\n",
    "    min_dist = np.min(dist_to_defenders, axis=1)\n",
    "    return min_dist # Actual Attention (A) or Expected Coverage (E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac6c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heliocentricity(play_data):\n",
    "    # Data is extracted from the evaluation loop's results_for_H_calc\n",
    "    Y_truth = play_data['Y_truth']       # (T_pred, N_agents, 2)\n",
    "    Y_pred_K = play_data['Y_pred_K']     # (K, T_pred, N_agents, 2)\n",
    "    star_idx = play_data['star_idx']     # Index of the star receiver\n",
    "    \n",
    "    # --- 1. Identify Offensive/Defensive Players ---\n",
    "    # Assuming player 0-10 are Offense, 11-21 are Defense (Adjust based on BDB index)\n",
    "    def_indices = np.arange(11, 22) \n",
    "    \n",
    "    # --- 2. Calculate Actual Attention (A) ---\n",
    "    actual_R_coords = Y_truth[:, star_idx, :]\n",
    "    actual_D_coords = Y_truth[:, def_indices, :]\n",
    "    A = min_separation_distance(actual_R_coords, actual_D_coords) # (T_pred,)\n",
    "    \n",
    "    # --- 3. Calculate Expected Coverage (E) ---\n",
    "    E_K = []\n",
    "    for k in range(Y_pred_K.shape[0]):\n",
    "        # The star receiver's true position is used, but compared to predicted defense\n",
    "        predicted_D_coords = Y_pred_K[k, :, def_indices, :] \n",
    "        \n",
    "        # E_k is the min separation based on the k-th predicted defensive trajectory\n",
    "        E_k = min_separation_distance(actual_R_coords, predicted_D_coords)\n",
    "        E_K.append(E_k)\n",
    "        \n",
    "    # E_mean: The average expected minimum separation across all K samples (T_pred,)\n",
    "    E_mean = np.mean(np.stack(E_K, axis=0), axis=0) \n",
    "    \n",
    "    # --- 4. Calculate Heliocentricity (H) ---\n",
    "    # H = (E - A) averaged over the prediction window (T_pred)\n",
    "    H_frame_diff = E_mean - A\n",
    "    H_score = np.mean(H_frame_diff)\n",
    "    \n",
    "    return H_score, H_frame_diff # Return both the scalar and the time-series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
